{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68b5d7b-0b32-4a3b-9914-a1918ace0950",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "> In machine learning, training algorithms are used to learn model parameters by minimizing a loss function. Some algorithms, like support vector classifiers and random forests, have user-defined hyperparameters that affect parameter learning.\n",
    "> > Parameters are learned during training, while hyperparameters are set manually. For instance, random forests consist of decision trees, but the number of trees must be predetermined.\n",
    "> > > This process is known as `hyperparameter tuning` or `model selection`. The goal is to choose the best learning algorithm and hyperparameters, leading to the best model. Various techniques are available to efficiently select the optimal model from a set of candidates in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7e08d1-28c4-450c-b448-dba2a4a18b70",
   "metadata": {},
   "source": [
    "#  Selecting the Best Models Using Exhaustive Search\n",
    "> `GridSearchCV` is a method for model selection through `cross-validation`.\n",
    "> > User defines possible hyperparameter values, and GridSearchCV trains models using all combinations.\n",
    "> > The best model is chosen based on performance score.\n",
    "> > > For example, we used logistic regression with hyperparameters C and regularization penalty, along with other parameters. Specific values must be set for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1666a8c0-a626-4f74-b9df-2f90ab8f4eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=7.742636826811269, max_iter=500, penalty='l1',\n",
      "                   solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "# If you want to select the best model by searching over a range of hyperparameters.\n",
    "# Use scikit-learn’s GridSearchCV:\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "# Create logistic regression\n",
    "logistic = linear_model.LogisticRegression(max_iter=500, solver='liblinear')\n",
    "# Create range of candidate penalty hyperparameter values\n",
    "penalty = ['l1','l2']\n",
    "# Create range of candidate regularization hyperparameter values\n",
    "C = np.logspace(0, 4, 10)\n",
    "# Create dictionary of hyperparameter candidates\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "# Create grid search\n",
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, verbose=0)\n",
    "# Fit grid search\n",
    "best_model = gridsearch.fit(features, target)\n",
    "# Show the best model\n",
    "print(best_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43b210d5-cfd0-47e9-b076-1edef993cb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+00, 2.78255940e+00, 7.74263683e+00, 2.15443469e+01,\n",
       "       5.99484250e+01, 1.66810054e+02, 4.64158883e+02, 1.29154967e+03,\n",
       "       3.59381366e+03, 1.00000000e+04])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(0, 4, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f33af58-e548-4de3-bda1-fb9cb26b97f9",
   "metadata": {},
   "source": [
    "- we define two possible values for the regularization penalty: ['l1', 'l2'].\n",
    "For each combination of C and regularization penalty values, we train the model\n",
    "and evaluate it using k-fold cross-validation. In our solution, we have 10 possible\n",
    "values of C, 2 possible values of regularization penalty, and 5 folds. They create\n",
    "10 × 2 × 5 = `100 candidate models`, from which the best is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e927f46-0246-45af-b3be-4a5deac36340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: l1\n",
      "Best C: 7.742636826811269\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV is complete, we can see the hyperparameters of the best model:\n",
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521dac89-f99a-4e95-a671-13ad26109518",
   "metadata": {},
   "source": [
    "- By default, after identifying the best hyperparameters, GridSearchCV will retrain a model using the best hyperparameters on the entire dataset (rather than leaving a fold out for cross-validation).\n",
    "- We can use this model to predict values like any other\n",
    "scikit-learn model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75a05c02-3140-4b9c-8854-5bf9ec4f077b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict target vector\n",
    "best_model.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba49b73-f5b1-4560-a62e-81638e031848",
   "metadata": {},
   "source": [
    "# Selecting the Best Models Using Randomized Search\n",
    "> Using RandomizedSearchCV with user-defined hyperparameter values is a more efficient method than GridSearchCV for finding the best model.\n",
    "> > Through `randomized sampling`, scikit-learn can search over specific distributions for optimal hyperparameter values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31301a88-7bad-4602-845f-ba19f08600dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want a computationally cheaper method than exhaustive search to select the best model.\n",
    "# Use scikit-learn’s RandomizedSearchCV:\n",
    "# Load libraries\n",
    "from scipy.stats import uniform\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "# Create logistic regression\n",
    "logistic = linear_model.LogisticRegression(max_iter=500, solver='liblinear')\n",
    "# Create range of candidate regularization penalty hyperparameter values\n",
    "penalty = ['l1', 'l2']\n",
    "# Create distribution of candidate regularization hyperparameter values\n",
    "C = uniform(loc=0, scale=4)\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "# Create randomized search\n",
    "randomizedsearch = RandomizedSearchCV(\n",
    " logistic, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0,\n",
    " n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12db8f64-f544-4f54-907d-3a9689aa208c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.668088018810296, max_iter=500, penalty='l1',\n",
      "                   solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "# Fit randomized search\n",
    "best_model = randomizedsearch.fit(features, target)\n",
    "# Print best model\n",
    "print(best_model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d437d1b-2622-4bbd-ade6-60f5242ebb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: l1\n",
      "Best C: 1.668088018810296\n"
     ]
    }
   ],
   "source": [
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b94ba00-dfab-4c1a-a8b4-92413561c294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict target vector\n",
    "best_model.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ba863-f449-4f0e-b73c-2597b4047983",
   "metadata": {},
   "source": [
    "- The number of sampled combinations of hyperparameters (i.e., the number of candidate models trained) is specified with the n_iter (number of iterations) setting.\n",
    "- It’s\n",
    "worth noting that RandomizedSearchCV isn’t inherently faster than GridSearchCV, but\n",
    "it often achieves comparable performance to GridSearchCV in less time just by testing\n",
    "fewer combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff291814-f05a-45a4-ba4e-73a4296a0468",
   "metadata": {},
   "source": [
    "# Selecting the Best Models from Multiple Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26cf5972-e00d-41e2-9ee4-204dd0f6dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to select the best model by searching over a range of learning algorithms and their respective hyperparameters.\n",
    "# Create a dictionary of candidate learning algorithms and their hyperparameters to use as the search space for GridSearchCV:\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "# Create a pipeline\n",
    "pipe = Pipeline([(\"classifier\", RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c64e264-2eea-421a-b910-f39f68d53d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('classifier',\n",
      "                 LogisticRegression(C=7.742636826811269, max_iter=500,\n",
      "                                    penalty='l1', solver='liblinear'))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaveh\\anaconda3\\envs\\GPU_Kaveh\\Lib\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary with candidate learning algorithms and their hyperparameters\n",
    "search_space = [{\"classifier\": [LogisticRegression(max_iter=500,solver='liblinear')],\n",
    "                 \"classifier__penalty\": ['l1', 'l2'],\n",
    "                 \"classifier__C\": np.logspace(0, 4, 10)},\n",
    "                 {\"classifier\": [RandomForestClassifier()],\n",
    "                 \"classifier__n_estimators\": [10, 100, 1000],\n",
    "                 \"classifier__max_features\": [1, 2, 3]}]\n",
    "# Create grid search\n",
    "gridsearch = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n",
    "# Fit grid search\n",
    "best_model = gridsearch.fit(features, target)\n",
    "# Print best model\n",
    "print(best_model.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f51c925-ad69-4fdc-afc6-f206b5f3fbd1",
   "metadata": {},
   "source": [
    "#### Each learning algorithm has its own hyperparameters, and we define their candidate values using the format `classifier__[hyperparameter name]`. \n",
    "> For example, for our logistic regression, to define the set of possible values for regularization hyperparameter space, C, and potential types of regularization penalties, penalty, we create a\n",
    "dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9d970af-7d88-4241-93b1-6eed4560aa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=7.742636826811269, max_iter=500, penalty='l1',\n",
      "                   solver='liblinear')\n"
     ]
    }
   ],
   "source": [
    "# After the search is complete, we can use best_estimator_ to view the best model’s learning algorithm and hyperparameters:\n",
    "# View best model\n",
    "print(best_model.best_estimator_.get_params()[\"classifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b58fe91-aca3-4a4f-a6a4-d978469285e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict target vector\n",
    "best_model.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa21b3-4d27-4a41-85fd-271125f94abf",
   "metadata": {},
   "source": [
    "# End of chapter 12"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU_Kaveh)",
   "language": "python",
   "name": "gpu_kaveh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
