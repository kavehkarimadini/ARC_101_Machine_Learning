{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e9c891-0c0e-4631-be98-9cd71a5d90ef",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "> It is often useful to measure objects not in terms of their quantity but in terms of\n",
    "some quality. We frequently represent qualitative information in categories such as\n",
    "gender, colors, or brand of car. However, not all categorical data is the same.\n",
    "> > Sets\n",
    "of categories with no intrinsic ordering are called `nominal`. Examples of nominal\n",
    "categories include:\n",
    "> > - Blue, Red, Green\n",
    "> > - Man, Woman\n",
    "> > - Banana, Strawberry, Apple\n",
    "\n",
    ">> In contrast, when a set of categories has some natural ordering we refer to it as\n",
    "`ordinal`. For example:\n",
    ">> - Low, Medium, High\n",
    ">> - Young, Old\n",
    ">> - Agree, Neutral, Disagree\n",
    "\n",
    "> - The problem is that most\n",
    "machine learning algorithms require inputs to be numerical values.\n",
    "> - The k-nearest neighbors algorithm is an example of an algorithm that requires\n",
    "numerical data.\n",
    ">> One step in the algorithm is calculating the distances between observations (often using Euclidean distance)\n",
    "\n",
    "> - we need to convert the string into some\n",
    "numerical format so that it can be input into the Euclidean distance equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cbcd3f-49f0-4f30-a3d2-a7a9c5d6348e",
   "metadata": {},
   "source": [
    "# Encoding Nominal Categorical Features\n",
    "> - We might think the proper strategy is to assign each class a numerical value (e.g.,\n",
    "Texas = 1, California = 2).\n",
    "> - However, when our classes have no intrinsic ordering\n",
    "(e.g., Texas isn’t “less” than California), our numerical values erroneously create an\n",
    "ordering that is not present.\n",
    ">> - The proper strategy is to create a binary feature for each class in the original feature.\n",
    "This is often called `one-hot encoding` (in machine learning literature) or `dummying`\n",
    "(in statistical and research literature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e82c4a05-5a43-423b-954b-ad2b3b1702e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have a feature with nominal classes that has no intrinsic ordering (e.g., apple,pear, banana), \n",
    "# and you want to encode the feature into numerical values, \n",
    "# you must One-hot encode the feature using scikit-learn’s LabelBinarizer:\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\n",
    "# Create feature\n",
    "feature = np.array([[\"Texas\"],\n",
    " [\"California\"],\n",
    " [\"Texas\"],\n",
    " [\"Delaware\"],\n",
    " [\"Texas\"]])\n",
    "# Create one-hot encoder\n",
    "one_hot = LabelBinarizer()\n",
    "# One-hot encode feature\n",
    "one_hot.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f0dff9e-93dc-40f4-80d3-60e4b2db7f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['California', 'Delaware', 'Texas'], dtype='<U10')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use the classes_ attribute to output the classes:\n",
    "# View feature classes\n",
    "one_hot.classes_\n",
    "# array(['California', 'Delaware', 'Texas'],dtype='<U10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d47eaad-9599-466c-b20d-41050e527099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Texas', 'California', 'Texas', 'Delaware', 'Texas'], dtype='<U10')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to reverse the one-hot encoding, we can use inverse_transform:\n",
    "# Reverse one-hot encoding\n",
    "one_hot.inverse_transform(one_hot.transform(feature))\n",
    "# array(['Texas', 'California', 'Texas', 'Delaware', 'Texas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b51df4fb-03c4-42eb-ab47-b58c58ca3fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>California</th>\n",
       "      <th>Delaware</th>\n",
       "      <th>Texas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   California  Delaware  Texas\n",
       "0       False     False   True\n",
       "1        True     False  False\n",
       "2       False     False   True\n",
       "3       False      True  False\n",
       "4       False     False   True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can even use pandas to one-hot encode the feature:\n",
    "# Import library\n",
    "import pandas as pd\n",
    "# Create dummy variables from feature\n",
    "pd.get_dummies(feature[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db38a6d4-f982-4ea2-838f-4a459e9a8dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1],\n",
       "       [1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One helpful feature of scikit-learn is the ability to handle a situation where each\n",
    "# observation lists multiple classes:\n",
    "# Create multiclass feature\n",
    "multiclass_feature = [(\"Texas\", \"Florida\"),\n",
    " (\"California\", \"Alabama\"),\n",
    " (\"Texas\", \"Florida\"),\n",
    " (\"Delaware\", \"Florida\"),\n",
    " (\"Texas\", \"Alabama\")]\n",
    "# Create multiclass one-hot encoder\n",
    "one_hot_multiclass = MultiLabelBinarizer()\n",
    "# One-hot encode multiclass feature\n",
    "one_hot_multiclass.fit_transform(multiclass_feature)\n",
    "# array([[0, 0, 0, 1, 1],\n",
    "#  [1, 1, 0, 0, 0],\n",
    "#  [0, 0, 0, 1, 1],\n",
    "#  [0, 0, 1, 1, 0],\n",
    "#  [1, 0, 0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bc5a7ed-2c97-4eb5-8e01-e5c57984a5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alabama', 'California', 'Delaware', 'Florida', 'Texas'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Once again, we can see the classes with the classes_ method:\n",
    "# View classes\n",
    "one_hot_multiclass.classes_\n",
    "# array(['Alabama', 'California', 'Delaware', 'Florida', 'Texas'], dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade563f-1e0b-4c6d-9eef-7f8b3ca972b0",
   "metadata": {},
   "source": [
    "# Encoding Ordinal Categorical Features\n",
    "> Often we have a feature with classes that have some kind of natural ordering.\n",
    ">> A famous example is the Likert scale:\n",
    ">>> Strongly Agree • Agree • Neutral • Disagree • Strongly Disagree\n",
    "\n",
    "> - When encoding the feature for use in machine learning, we need to transform the\n",
    "ordinal classes into numerical values that maintain the notion of ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99e5d4fb-2552-4bdf-90aa-5d56f6c671b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    2\n",
       "3    2\n",
       "4    3\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have an ordinal categorical feature (e.g., high, medium, low), \n",
    "# and you want to transform it into numerical values, \n",
    "# use the pandas DataFrame replace method to transform string labels to numerical equivalents:\n",
    "# Load library\n",
    "import pandas as pd\n",
    "# Create features\n",
    "dataframe = pd.DataFrame({\"Score\": [\"Low\", \"Low\", \"Medium\", \"Medium\", \"High\"]})\n",
    "# Create mapper\n",
    "scale_mapper = {\"Low\":1,\n",
    " \"Medium\":2,\n",
    " \"High\":3}\n",
    "# Replace feature values with scale\n",
    "dataframe[\"Score\"].replace(scale_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b7946d1-1af6-4fa8-a7b4-31cc2b3a20cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    2\n",
       "3    2\n",
       "4    4\n",
       "5    3\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is important that our choice of numeric values is based on our prior information on\n",
    "# the ordinal classes. In our solution, high is literally three times larger than low. This\n",
    "# is fine in many instances but can break down if the assumed intervals between the classes are not equal:\n",
    "dataframe = pd.DataFrame({\"Score\": [\"Low\",\n",
    "                                    \"Low\",\n",
    "                                    \"Medium\",\n",
    "                                    \"Medium\",\n",
    "                                    \"High\",\n",
    "                                    \"Barely More Than Medium\"]})\n",
    "scale_mapper = {\"Low\":1,\n",
    "                \"Medium\":2,\n",
    "                \"Barely More Than Medium\":3,\n",
    "                \"High\":4}\n",
    "dataframe[\"Score\"].replace(scale_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0272ebb3-cb04-4aa6-854e-1d0a193a16d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    1.0\n",
       "2    2.0\n",
       "3    2.0\n",
       "4    3.0\n",
       "5    2.1\n",
       "Name: Score, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct way:\n",
    "# The best approach is to be conscious about the numerical values mapped to classes:\n",
    "scale_mapper = {\"Low\":1,\n",
    "                \"Medium\":2,\n",
    "                \"Barely More Than Medium\":2.1,\n",
    "                \"High\":3}\n",
    "dataframe[\"Score\"].replace(scale_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925c75ba-55aa-4ec2-9998-2f67f1af3b55",
   "metadata": {},
   "source": [
    "# Encoding Dictionaries of Features\n",
    "> - This is a common situation when working with natural language processing.\n",
    "> - For example, we might have a collection of documents and for each document we have\n",
    "a dictionary containing the number of times every word appears in the document.\n",
    ">> Using `DictVectorizer`, we can easily create a feature matrix where every feature is\n",
    "the number of times a word appears in each document:\n",
    ">>> 1. Create word count dictionaries for four documents\n",
    ">>>`doc_1_word_count = {\"Red\": 2, \"Blue\": 4}` <br>\n",
    "`doc_2_word_count = {\"Red\": 4, \"Blue\": 3}`<br>\n",
    "`doc_3_word_count = {\"Red\": 1, \"Yellow\": 2}`<br>\n",
    "`doc_4_word_count = {\"Red\": 2, \"Yellow\": 2}`<br><br>\n",
    ">>> 2. Create list\n",
    "`doc_word_counts = [doc_1_word_count,\n",
    " doc_2_word_count,\n",
    " doc_3_word_count,\n",
    " doc_4_word_count]`<br>\n",
    ">>> 3. Convert list of word count dictionaries into feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7848d313-3be3-4ae3-b1c3-99d3771e2022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 2., 0.],\n",
       "       [3., 4., 0.],\n",
       "       [0., 1., 2.],\n",
       "       [0., 2., 2.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have a dictionary and want to convert it into a feature matrix, use DictVectorizer:\n",
    "# Import library\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# Create dictionary\n",
    "data_dict = [{\"Red\": 2, \"Blue\": 4},\n",
    "             {\"Red\": 4, \"Blue\": 3},\n",
    "             {\"Red\": 1, \"Yellow\": 2},\n",
    "             {\"Red\": 2, \"Yellow\": 2}]\n",
    "\n",
    "# Create dictionary vectorizer\n",
    "dictvectorizer = DictVectorizer(sparse=False)\n",
    "# Convert dictionary to feature matrix\n",
    "features = dictvectorizer.fit_transform(data_dict)\n",
    "# View feature matrix\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8ae90c-0905-4612-b38e-b91589085578",
   "metadata": {},
   "source": [
    "- By default `DictVectorizer` outputs a `sparse matrix` that only stores elements with\n",
    "a value other than 0. This can be very helpful when we have massive matrices\n",
    "(often encountered in natural language processing) and want to minimize the memory requirements.\n",
    "- We can force `DictVectorizer` to output a dense matrix using\n",
    "`sparse=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d819ca0-4af2-4283-a014-14dba2a4e51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Blue', 'Red', 'Yellow']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can get the names of each generated feature using the get_feature_names method:\n",
    "# Get feature names\n",
    "feature_names = dictvectorizer.feature_names_\n",
    "# View feature names\n",
    "feature_names\n",
    "# ['Blue', 'Red', 'Yellow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "940f3bd3-60ee-4ba6-b4eb-ecfb5ae86d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Blue</th>\n",
       "      <th>Red</th>\n",
       "      <th>Yellow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Blue  Red  Yellow\n",
       "0   4.0  2.0     0.0\n",
       "1   3.0  4.0     0.0\n",
       "2   0.0  1.0     2.0\n",
       "3   0.0  2.0     2.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# While not necessary, for the sake of illustration we can create a pandas DataFrame to\n",
    "# view the output better:\n",
    "# Import library\n",
    "import pandas as pd\n",
    "# Create dataframe from features\n",
    "pd.DataFrame(features, columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b16f39-50dd-4102-a779-6b936f2d27f9",
   "metadata": {},
   "source": [
    "- In the above toy example there are only three unique words (Red, Yellow, Blue) so there\n",
    "are only three features in our matrix;\n",
    "- However, you can imagine that if each document\n",
    "was actually a book in a university library our feature matrix would be very large (and\n",
    "then we would want to set sparse to True)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a24bd9-c230-47af-99c3-a493356a4e7b",
   "metadata": {},
   "source": [
    "# Imputing Missing Class Values\n",
    "> best solution is to open\n",
    "our toolbox of machine learning algorithms to predict the values of the missing\n",
    "observations.\n",
    "> > We can accomplish this by treating the feature with the missing values\n",
    "as the target vector and the other features as the feature matrix.\n",
    "> > >A commonly used\n",
    "algorithm is KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22e2f606-c830-40df-9bc9-a81937774ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.87,  1.31],\n",
       "       [ 1.  , -0.67, -0.22],\n",
       "       [ 0.  ,  2.1 ,  1.45],\n",
       "       [ 1.  ,  1.18,  1.33],\n",
       "       [ 0.  ,  1.22,  1.27],\n",
       "       [ 1.  , -0.21, -1.19]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have a categorical feature containing missing values that you want to replace with predicted values.\n",
    "# Then the ideal solution is to train a machine learning classifier algorithm to predict the\n",
    "# missing values, commonly a k-nearest neighbors (KNN) classifier:\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Create feature matrix with categorical feature\n",
    "X = np.array([[0, 2.10, 1.45],\n",
    " [1, 1.18, 1.33],\n",
    " [0, 1.22, 1.27],\n",
    " [1, -0.21, -1.19]])\n",
    "# Create feature matrix with missing values in the categorical feature\n",
    "X_with_nan = np.array([[np.nan, 0.87, 1.31],\n",
    " [np.nan, -0.67, -0.22]])\n",
    "# Train KNN learner\n",
    "clf = KNeighborsClassifier(3, weights='distance')\n",
    "trained_model = clf.fit(X[:,1:], X[:,0])\n",
    "# Predict class of missing values\n",
    "imputed_values = trained_model.predict(X_with_nan[:,1:])\n",
    "# Join column of predicted class with their other features\n",
    "X_with_imputed = np.hstack((imputed_values.reshape(-1,1), X_with_nan[:,1:]))\n",
    "# Join two feature matrices\n",
    "np.vstack((X_with_imputed, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d09eaa06-0375-43fe-930b-81514bc98e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.87,  1.31],\n",
       "       [ 0.  , -0.67, -0.22],\n",
       "       [ 0.  ,  2.1 ,  1.45],\n",
       "       [ 1.  ,  1.18,  1.33],\n",
       "       [ 0.  ,  1.22,  1.27],\n",
       "       [ 1.  , -0.21, -1.19]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An alternative solution is to fill in missing values with the feature’s most frequent value:\n",
    "from sklearn.impute import SimpleImputer\n",
    "# Join the two feature matrices\n",
    "X_complete = np.vstack((X_with_nan, X))\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "imputer.fit_transform(X_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca1f43f-246f-4f41-b973-7126c084b644",
   "metadata": {},
   "source": [
    "# Handling Imbalanced Classes\n",
    "> - Our best strategy is simply to collect more observations—especially observations\n",
    "from the minority class. However, often this is just not possible, so we have to resort\n",
    "to other options.\n",
    ">________________________________________\n",
    ">> - A second strategy is to use a model evaluation metric better suited to imbalanced\n",
    "classes.\n",
    ">>> - Accuracy is often used as a metric for evaluating the performance of a model,\n",
    "but when imbalanced classes are present, accuracy can be ill suited. For example,\n",
    "if only 0.5% of observations have some rare cancer, then even a naive model that\n",
    "predicts nobody has cancer will be 99.5% accurate. Clearly this is not ideal. Some\n",
    "better metrics we discuss in later chapters are confusion matrices, precision, recall, F1\n",
    "scores, and ROC curves.\n",
    ">>___________________________________________\n",
    ">> - A third strategy is to use the class weighing parameters included in implementations\n",
    "of some models. This allows the algorithm to adjust for imbalanced classes. Fortu‐\n",
    "nately, many scikit-learn classifiers have a class_weight parameter, making it a good\n",
    "option.\n",
    ">>__________________________________________\n",
    ">> - The fourth and fifth strategies are related: downsampling and upsampling. In down‐\n",
    "sampling we create a random subset of the majority class of equal size to the minority\n",
    "class.\n",
    ">>> - In upsampling we repeatedly sample with replacement from the minority class\n",
    "to make it of equal size as the majority class.\n",
    ">>> - The decision between using downsampling and upsampling is context-specific, and in general we should try both to see\n",
    "which produces better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2976e713-f269-486b-99b4-720a621f84ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have a target vector with highly imbalanced classes, \n",
    "# and you want to make adjustments so that you can handle the class imbalance.\n",
    "# 1- Collect more data. \n",
    "# 2- If that isn’t possible, change the metrics used to evaluate your\n",
    "# model. \n",
    "# 3- If that doesn’t work, consider using a model’s built-in class weight parame‐\n",
    "# ters (if available), downsampling, or upsampling. We cover evaluation metrics in a\n",
    "# later chapter, so for now let’s focus on class weight parameters, downsampling, and\n",
    "# upsampling.\n",
    "# ---------------------------------------------------------------------------------------\n",
    "# To demonstrate our solutions, we need to create some data with imbalanced classes.\n",
    "# Fisher’s Iris dataset contains three balanced classes of 50 observations, each indicating\n",
    "# the species of flower (Iris setosa, Iris virginica, and Iris versicolor).\n",
    "\n",
    "# The result is 10 observations of Iris setosa (class 0) and 100 observations of not Iris setosa (class 1):\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "# Load iris data\n",
    "iris = load_iris()\n",
    "# Create feature matrix\n",
    "features = iris.data\n",
    "# Create target vector\n",
    "target = iris.target\n",
    "# Remove first 40 observations\n",
    "features = features[40:,:]\n",
    "target = target[40:]\n",
    "# Create binary target vector indicating if class 0\n",
    "target = np.where((target == 0), 0, 1)\n",
    "# Look at the imbalanced target vector\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21e4d146-32b1-4ee2-b7c5-20fe2a3f7133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight={0: 0.9, 1: 0.1})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight={0: 0.9, 1: 0.1})</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(class_weight={0: 0.9, 1: 0.1})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Many algorithms in scikit-learn offer a parameter to weight classes during training\n",
    "# to counteract the effect of their imbalance. \n",
    "# Random ForestClassifier is a popular classification algorithm and includes a class_weight parameter\n",
    "# You can pass an argument explicitly specifying the desired class weights:\n",
    "# Create weights\n",
    "weights = {0: 0.9, 1: 0.1}\n",
    "# Create random forest classifier with weights\n",
    "RandomForestClassifier(class_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "757e01cd-2ac5-4ec8-a91e-23efcfe56ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can pass balanced, which automatically creates weights inversely propor‐\n",
    "# tional to class frequencies:\n",
    "# Train a random forest with balanced class weights\n",
    "RandomForestClassifier(class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c44efaa-47e8-48f5-a574-8f0f4557067d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternatively, we can downsample the majority class or upsample the minority class.\n",
    "# In downsampling, we randomly sample without replacement from the majority class\n",
    "# (i.e., the class with more observations) to create a new subset of observations equal\n",
    "# in size to the minority class:\n",
    "\n",
    "# Indicies of each class's observations\n",
    "i_class0 = np.where(target == 0)[0]\n",
    "i_class1 = np.where(target == 1)[0]\n",
    "# Number of observations in each class\n",
    "n_class0 = len(i_class0)\n",
    "n_class1 = len(i_class1)\n",
    "# For every observation of class 0, randomly sample\n",
    "# from class 1 without replacement\n",
    "i_class1_downsampled = np.random.choice(i_class1, size=n_class0, replace=False)\n",
    "# Join together class 0's target vector with the\n",
    "# downsampled class 1's target vector\n",
    "np.hstack((target[i_class0], target[i_class1_downsampled]))\n",
    "# Join together class 0's feature matrix with the\n",
    "# downsampled class 1's feature matrix\n",
    "np.vstack((features[i_class0,:], features[i_class1_downsampled,:]))[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a7913cc-79cc-44b3-9e94-190b93ebcdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5. , 3.5, 1.3, 0.3],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [5.1, 3.8, 1.6, 0.2]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our other option is to upsample the minority class. \n",
    "# In upsampling, for every observation in the majority class,\n",
    "# we randomly select an observation from the minority class with replacement.\n",
    "# The result is the same number of observations from the minority and majority classes. \n",
    "# Upsampling is implemented very similarly to downsampling, just in reverse:\n",
    "# For every observation in class 1, randomly sample from class 0 with\n",
    "# replacement\n",
    "i_class0_upsampled = np.random.choice(i_class0, size=n_class1, replace=True)\n",
    "# Join together class 0's upsampled target vector with class 1's target vector\n",
    "np.concatenate((target[i_class0_upsampled], target[i_class1]))\n",
    "# Join together class 0's upsampled feature matrix with class 1's feature matrix\n",
    "np.vstack((features[i_class0_upsampled,:], features[i_class1,:]))[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b701549a-ada3-4443-bd9a-817c7077edc1",
   "metadata": {},
   "source": [
    "# END of Chapter 5 --> Handling Categorical Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
