{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d32650-63f9-456f-a60c-a3ea277fb288",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "> Feature Selection:\n",
    "> > selecting high-quality, informative features and dropping less useful features.\n",
    "> ______________________________\n",
    "> Three types of feature selection methods:\n",
    "> 1. `Filter methods` select the best features by examining their statistical properties.\n",
    "> > Methods where we explicitly set a threshold for a statistic or manually select the number\n",
    "of features we want to keep are examples of feature selection by filtering.\n",
    "> 2. `Wrapper methods` use trial and error to find the subset of features that produces models with\n",
    "the highest quality predictions.\n",
    "> > Wrapper methods are often the most effective, as they find the best result through actual experimentation as opposed to naive assumptions.\n",
    "> 3. `Embedded methods` select the best feature subset as part of, as an extension of, a learning algorithm’s training process."
   ]
  },
  {
   "attachments": {
    "a202bd67-3869-4da1-9dd4-7f58e0efb7ad.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "933d3152-2ff3-402d-91da-fa9fbcfccd66",
   "metadata": {},
   "source": [
    "# Thresholding Numerical Feature Variance\n",
    "> `Variance thresholding (VT)` is an example of feature selection by filtering, and one of\n",
    "the most basic approaches to feature selection. It is motivated by the idea that features\n",
    "with low variance are likely less interesting (and less useful) than features with high\n",
    "variance.\n",
    "> > VT first calculates the variance of each feature:<br>\n",
    "> > ![image.png](attachment:a202bd67-3869-4da1-9dd4-7f58e0efb7ad.png)\n",
    "\n",
    "> > where x is the feature vector, xi is an individual feature value, and μ is that feature’s\n",
    "mean value. ---> It drops all features whose variance does not meet that threshold.\n",
    "> ________________________________________\n",
    "> 2 key point:\n",
    "> > 1. The variance is not centered (it is in the squared unit of the feature itself), so VT will not work\n",
    "when feature sets contain different units (e.g., one feature is in years while another is in dollars).\n",
    ">> 2. The variance threshold is selected manually, so we have to use our own judgment for a good value to select \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e32c0c0-e1ff-4f02-bcb0-a17494d37121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 1.4, 0.2],\n",
       "       [4.9, 1.4, 0.2],\n",
       "       [4.7, 1.3, 0.2]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have a set of numerical features and want to filter out those with low variance(i.e., likely containing little information),\n",
    "# just select a subset of features with variances above a given threshold:\n",
    "# Load libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "# Import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "# Create features and target\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "# Create thresholder\n",
    "thresholder = VarianceThreshold(threshold=.5)\n",
    "# Create high variance feature matrix\n",
    "features_high_variance = thresholder.fit_transform(features)\n",
    "# View high variance feature matrix\n",
    "features_high_variance[0:3]\n",
    "# array([[ 5.1, 1.4, 0.2],\n",
    "#  [ 4.9, 1.4, 0.2],\n",
    "#  [ 4.7, 1.3, 0.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e188b7e-8fed-496b-a2f3-c85cb4118333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68112222, 0.18871289, 3.09550267, 0.57713289])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see the variance for each feature using variances_:\n",
    "# View variances\n",
    "thresholder.fit(features).variances_\n",
    "# array([0.68112222, 0.18871289, 3.09550267, 0.57713289])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fac7d32-1a2a-47ba-8cdb-be7865e8ce5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the features have been standardized (to mean zero and unit variance), then\n",
    "# for obvious reasons VT will not work correctly:\n",
    "# Load library\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize feature matrix\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "# Caculate variance of each feature\n",
    "selector = VarianceThreshold()\n",
    "selector.fit(features_std).variances_\n",
    "# array([1., 1., 1., 1.])"
   ]
  },
  {
   "attachments": {
    "c7effb8c-8211-4d72-9408-3b4898357ce1.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "86541061-6953-409c-b1f1-f53b720ff6e5",
   "metadata": {},
   "source": [
    "# Thresholding Binary Feature Variance\n",
    "> As with numerical features, one strategy for selecting highly informative categorical\n",
    "features and filtering out less informative ones is to examine their variances.\n",
    "> > In binary features (i.e., Bernoulli random variables), variance is calculated as:<br>\n",
    ">> ![image.png](attachment:c7effb8c-8211-4d72-9408-3b4898357ce1.png)\n",
    "\n",
    ">> - Where p is the proportion of observations of class 1. Therefore, by setting p, we can\n",
    "remove features where the vast majority of observations are one class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce974d5e-cbbf-4bb3-881b-10ab22bcd1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have a set of binary categorical features and want to filter out those with low variance (likely containing little information).\n",
    "# just select a subset of features with a Bernoulli random variable variance above a given threshold:\n",
    "# Load library\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "# Create feature matrix with:\n",
    "# Feature 0: 80% class 0\n",
    "# Feature 1: 80% class 1\n",
    "# Feature 2: 60% class 0, 40% class 1\n",
    "features = [[0, 1, 0],\n",
    "         [0, 1, 1],\n",
    "         [0, 1, 0],\n",
    "         [0, 1, 1],\n",
    "         [1, 0, 0]]\n",
    "# Run threshold by variance\n",
    "thresholder = VarianceThreshold(threshold=(.75 * (1 - .75)))\n",
    "thresholder.fit_transform(features)\n",
    "# array([[0],\n",
    "#  [1],\n",
    "#  [0],\n",
    "#  [1],\n",
    "#  [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e7febe-24fb-4aec-9ff5-0e3be5207aa4",
   "metadata": {},
   "source": [
    "# Handling Highly Correlated Features\n",
    "> One problem we often run into in machine learning is highly correlated features.\n",
    "> > If two features are highly correlated, then the information they contain is very similar,\n",
    "and it is likely redundant to include both features.<br>\n",
    "> > In the case of simple models like linear regression, failing to remove such features violates the assumptions of linear\n",
    "regression and can result in an `artificially inflated R-squared value`.\n",
    "\n",
    "\n",
    "> > The solution to highly correlated features is simple:\n",
    "> > > remove one of them from the feature set. you can do it by setting a correlation threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b08f1c-d8cd-48cb-93bb-34663b4ed40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  2\n",
       "0  1  1\n",
       "1  2  0\n",
       "2  3  1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have a feature matrix and suspect some features are highly correlated,\n",
    "# use a correlation matrix to check for highly correlated features. \n",
    "# If highly correlated features exist, consider dropping one of the correlated features:\n",
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Create feature matrix with two highly correlated features\n",
    "features = np.array([[1, 1, 1],\n",
    "                     [2, 2, 0],\n",
    "                     [3, 3, 1],\n",
    "                     [4, 4, 0],\n",
    "                     [5, 5, 1],\n",
    "                     [6, 6, 0],\n",
    "                     [7, 7, 1],\n",
    "                     [8, 7, 0],\n",
    "                     [9, 7, 1]])\n",
    "\n",
    "# Convert feature matrix into DataFrame\n",
    "dataframe = pd.DataFrame(features)\n",
    "# Create correlation matrix\n",
    "corr_matrix = dataframe.corr().abs()\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),\n",
    " k=1).astype(bool))\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "# Drop features\n",
    "dataframe.drop(dataframe.columns[to_drop], axis=1).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c987ce-0aa1-401e-982f-bdbadf5796d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976103</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.976103</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.034503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.034503</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  1.000000  0.976103  0.000000\n",
       "1  0.976103  1.000000 -0.034503\n",
       "2  0.000000 -0.034503  1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In above code, first we create a correlation matrix of all features:\n",
    "# Correlation matrix\n",
    "dataframe.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "143f720c-1351-46f8-8010-3e18984ffa18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.976103</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.034503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2\n",
       "0 NaN  0.976103  0.000000\n",
       "1 NaN       NaN  0.034503\n",
       "2 NaN       NaN       NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second, we look at the upper triangle of the correlation matrix to identify pairs of highly correlated features:\n",
    "# Upper triangle of correlation matrix\n",
    "upper\n",
    "# Third, we remove one feature from each of those pairs."
   ]
  },
  {
   "attachments": {
    "fb68ccd5-1fa9-4408-bdd5-ad975c034c79.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "b84cfacc-1d5c-44e3-ac6b-eb88f9210b87",
   "metadata": {},
   "source": [
    "# Removing Irrelevant Features for Classification\n",
    "> `Chi-square` statistics examine the independence of two categorical vectors.\n",
    "> > The statistic is the difference between the observed number of observations in each\n",
    "class of a categorical feature and what we would expect if that feature were independent (no relationship) of the target vector:<br><br>\n",
    "> > ![image.png](attachment:fb68ccd5-1fa9-4408-bdd5-ad975c034c79.png)\n",
    "\n",
    "> > where Oi is the number of observed observations in class i, and Ei is the number of expected observations in class i.\n",
    ">__________________________________________________\n",
    "> A chi-squared statistic is a single number that tells you how much difference exists\n",
    "between your observed counts and the counts you would expect if there were no relationship at all in the population.\n",
    "> > By calculating the chi-squared statistic between a feature and the target vector, we obtain a measurement of the independence between\n",
    "the two.\n",
    "> > > - If the target is independent of the feature variable, then it is irrelevant for\n",
    "our purposes because it contains no information we can use for classification.\n",
    ">>> - On the other hand, if the two features are highly dependent, they likely are very informative for training our model.\n",
    ">___________________________________________________\n",
    "> To use chi-squared in feature selection, we `calculate the chi-squared` statistic `between`\n",
    "each `feature` and the `target` vector, then select the features with the best chi-square statistics.\n",
    "> > In `scikit-learn`, we can use `SelectKBest` to select them.\n",
    "> > >The parameter `k` determines the number of features we want to keep and filters out the least informative features.\n",
    ">___________________________________________\n",
    "> **Note:**\n",
    "> 1. Chi-square statistics can be calculated only between two categorical vectors (both the target vector and the features must be categorical).\n",
    "> > If we have a numerical feature we can use the chi-squared technique by first transforming\n",
    "the quantitative feature into a categorical feature.\n",
    "> 2. To use our chi-squared approach, all values need to be `non negative`.\n",
    ">_____________________________________________\n",
    "> Numerical feature:\n",
    "> >We can use `f_classif` to calculate the `ANOVA F-value statistic` with each feature and the target vector.\n",
    "> >> `F-value` scores examine if, when we group the numerical feature by the target vector, the means for\n",
    "each group are significantly different.\n",
    "> >> > **Example:** if we had a binary target vector, gender, and a quantitative feature, test scores, the F-value score would tell us if the\n",
    "mean test score for men is different than the mean test score for women. If it is not, then test score doesn’t help us predict gender and therefore the feature is irrelevant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7951ee41-d447-4e26-844f-85a885e872ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 4\n",
      "Reduced number of features: 2\n"
     ]
    }
   ],
   "source": [
    "# If you have a categorical target vector and want to remove uninformative features,\n",
    "# for the features that are categorical, \n",
    "# just calculate a chi-square (χ2) statistic between each feature and the target vector:\n",
    "# Load libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "# Convert to categorical data by converting data to integers\n",
    "features = features.astype(int)\n",
    "# Select two features with highest chi-squared statistics\n",
    "chi2_selector = SelectKBest(chi2, k=2)\n",
    "features_kbest = chi2_selector.fit_transform(features, target)\n",
    "# Show results\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dd2940c-e647-4541-9b8f-fe98010025b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 4\n",
      "Reduced number of features: 2\n"
     ]
    }
   ],
   "source": [
    "# If the features are quantitative, \n",
    "# compute the ANOVA F-value between each feature and the target vector:\n",
    "# Select two features with highest F-values\n",
    "fvalue_selector = SelectKBest(f_classif, k=2)\n",
    "features_kbest = fvalue_selector.fit_transform(features, target)\n",
    "# Show results\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_kbest.shape[1])\n",
    "# Original number of features: 4\n",
    "# Reduced number of features: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bd77054-e0c0-4d5d-80fb-1d745e463b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 4\n",
      "Reduced number of features: 3\n"
     ]
    }
   ],
   "source": [
    "# Instead of selecting a specific number of features,\n",
    "# we can use SelectPercentile to select the top n percent of features:\n",
    "# Load library\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "# Select top 75% of features with highest F-values\n",
    "fvalue_selector = SelectPercentile(f_classif, percentile=75)\n",
    "features_kbest = fvalue_selector.fit_transform(features, target)\n",
    "# Show results\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b868fd87-f845-4a41-9156-478d1bcfa82d",
   "metadata": {},
   "source": [
    "# Recursively Eliminating Features\n",
    "> The idea behind RFE is to train a model repeatedly, updating the weights or\n",
    "coefficients of that model each time.\n",
    "> > The first time we train the model, we include all the features. Then, we find the feature with the smallest parameter (notice that this\n",
    "assumes the features are either rescaled or standardized), meaning it is less important, and remove that feature from the feature set.\n",
    "> ___________________________________________\n",
    "> We can use `CV` to find the optimum number of features to keep during RFE.\n",
    "> In RFE with CV, after every iteration we use cross-validation to evaluate our model.\n",
    "> > If CV shows that our model improved after we eliminated a feature, then we continue on to the next loop. However, if CV shows that our model got worse after we eliminated a feature, we put that feature back into the feature set and select those features as the best.\n",
    ">_______________________________\n",
    "> In `scikit-learn`, `RFE with CV` is implemented using `RFECV`, which contains a number\n",
    "of important parameters:\n",
    "> > 1. The `estimator` parameter determines the type of model we want to train (e.g., linear regression),\n",
    "> > 2. the `step` parameter sets the number or proportion of features to drop during each loop,\n",
    "> > 3. the `scoring` parameter sets the metric of quality we use to evaluate our model during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97dd34bf-0a77-46a1-9784-b971f64d04f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00850799,  0.7031277 ,  1.02069177],\n",
       "       [-1.07500204,  2.56148527,  0.10585966],\n",
       "       [ 1.37940721, -1.77039484, -0.53049556],\n",
       "       ...,\n",
       "       [-0.80331656, -1.60648007,  0.25921194],\n",
       "       [ 0.39508844, -1.34564911, -1.80744499],\n",
       "       [-0.55383035,  0.82880112,  0.76876009]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to automatically select the best features to keep,\n",
    "# use scikit-learn’s RFECV to conduct recursive feature elimination (RFE) using cross-validation (CV). \n",
    "# That is, use the wrapper feature selection method and repeatedly train a model, each time removing a feature,\n",
    "# until model performance (e.g., accuracy) becomes worse,\n",
    "# the remaining features are the best:\n",
    "# Load libraries\n",
    "import warnings\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import datasets, linear_model\n",
    "# Suppress an annoying but harmless warning\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\",\n",
    " message=\"^internal gelsd\")\n",
    "# Generate features matrix, target vector, and the true coefficients\n",
    "features, target = make_regression(n_samples = 10000,\n",
    "                                 n_features = 100,\n",
    "                                 n_informative = 2,\n",
    "                                 random_state = 1)\n",
    "# Create a linear regression\n",
    "ols = linear_model.LinearRegression()\n",
    "# Recursively eliminate features\n",
    "rfecv = RFECV(estimator=ols, step=1, scoring=\"neg_mean_squared_error\")\n",
    "rfecv.fit(features, target)\n",
    "rfecv.transform(features)\n",
    "# array([[ 0.00850799, 0.7031277 , 1.52821875],\n",
    "#  [-1.07500204, 2.56148527, -0.44567768],\n",
    "#  [ 1.37940721, -1.77039484, -0.74675125],\n",
    "#  ...,\n",
    "#  [-0.80331656, -1.60648007, 0.52231601],\n",
    "#  [ 0.39508844, -1.34564911, 0.4228057 ],\n",
    "#  [-0.55383035, 0.82880112, 1.73232647]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0771499c-d9b1-4ff4-8ee4-7402fa8944b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Once we have conducted RFE, we can see the number of features we should keep:\n",
    "# Number of best features\n",
    "rfecv.n_features_\n",
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e935718-ca58-4e56-9133-6a3b52646fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also see which of those features we should keep:\n",
    "# Which categories are best\n",
    "rfecv.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ea715b6-bab0-428d-837a-8e1444b555b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([55, 54, 53, 17, 97,  1, 69, 12, 26, 58, 49, 35, 14,  9, 42, 24, 73,\n",
       "        6, 93, 32, 75, 19, 65, 45, 20, 23, 59, 91, 74, 29, 41, 16,  5, 88,\n",
       "       57, 90, 43, 86, 50,  1, 30, 47,  2, 76, 48, 60, 28, 40, 51, 37, 15,\n",
       "        8, 31, 83, 98,  4, 96, 70, 67, 56, 71, 10, 25, 66, 89, 46, 63, 33,\n",
       "       34, 92,  7, 11, 82, 87, 68, 72, 39, 61, 22, 81,  3, 78, 27, 38, 44,\n",
       "       18, 94, 62, 79, 36, 95, 13, 80, 52, 85, 84,  1, 21, 77, 64])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can even view the rankings of the features:\n",
    "# Rank features best (1) to worst\n",
    "rfecv.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42807999-4ff1-4336-b5c8-5ec8a3e32fb0",
   "metadata": {},
   "source": [
    "# END of chapter 10 --> dimension reduction using feature selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_Kaveh",
   "language": "python",
   "name": "gpu_kaveh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
