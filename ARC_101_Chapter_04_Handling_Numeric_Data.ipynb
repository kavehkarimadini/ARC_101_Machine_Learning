{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f6e67b-ad59-4cf0-9491-e4e50c453048",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "> - Quantitative data is the measurement of something—whether class size, monthly\n",
    "sales, or student scores.\n",
    "> - We will cover numerous strate‐\n",
    "gies for transforming raw numerical data into features purpose-built for machine\n",
    "learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e7142-0fd7-403e-9e6b-894c3dc27dc5",
   "metadata": {},
   "source": [
    "# Rescaling a Feature\n",
    "> - Rescaling is a common preprocessing task in machine learning.\n",
    "> - Many of the algorithms will assume all features are on the same scale,\n",
    "typically 0 to 1 or –1 to 1.\n",
    "> - There are a number of rescaling techniques, but one of\n",
    "the simplest is called `min-max scaling`.\n",
    ">> - Min-max scaling uses the minimum and maximum values of a feature to rescale values to within a range:<br>\n",
    "`xi′ = (xi − min x)/(max x − min x)`\n",
    ">>> where x is the feature vector, xi is an individual element of feature x, and xi′ is\n",
    "the rescaled element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14c88eb0-5879-45b7-bdf9-3c782c506e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.28571429],\n",
       "       [0.35714286],\n",
       "       [0.42857143],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To rescale the values of a numerical feature to be between two values, \n",
    "# use scikit-learn’s MinMaxScaler to rescale a feature array:\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "# Create feature\n",
    "feature = np.array([[-500.5],\n",
    " [-100.1],\n",
    " [0],\n",
    " [100.1],\n",
    " [900.9]])\n",
    "# Create scaler\n",
    "minmax_scale = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "minmax_scale.fit_transform(feature)\n",
    "# In our example, we can see from the outputted array that the feature has been successfully rescaled to between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1229c48-3863-4cb0-8949-1dadbd157b78",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "> 1. One option is to use `fit` to calculate the minimum and maximum values of the feature, and then\n",
    "use `transform` to rescale the feature.\n",
    "> 2. The second option is to use `fit_transform` to\n",
    "do both operations at once.\n",
    "- There is no mathematical difference between the two\n",
    "options, but there is sometimes a practical benefit to keeping the operations separate\n",
    "because it allows us to apply the same transformation to different sets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3a6b44-6af0-4b26-9b44-d97969517d0f",
   "metadata": {},
   "source": [
    "# Standardizing a Feature\n",
    "> - A common alternative to the `min-max scaling`  is rescaling of\n",
    "features to be approximately standard normally distributed.\n",
    ">> - To achieve this, we use standardization to transform the data such that it has a mean, x, of 0 and a standard\n",
    "deviation, σ, of 1:<br>\n",
    "`xi′ = (xi − x)/σ`\n",
    ">>> where xi′ is our standardized form of xi. The transformed feature represents the\n",
    "number of standard deviations of the original value from the feature’s mean value\n",
    "(also called a z-score in statistics).\n",
    ">______________________________________\n",
    "> - Standardization is used more often than min-max scaling. However, it\n",
    "depends on the learning algorithm. \n",
    ">> Example:<br>\n",
    ">>> 1. principal component analysis often\n",
    "works better using standardization\n",
    ">>> 2. min-max scaling is often recommended for\n",
    "neural networks \n",
    ">______________________________________________\n",
    "> **Note:** As a general rule, it is recommend defaulting to standardization unless you have a specific reason to use\n",
    "an alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0742799-5098-42cc-9429-50cf5729045c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76058269],\n",
       "       [-0.54177196],\n",
       "       [-0.35009716],\n",
       "       [-0.32271504],\n",
       "       [ 1.97516685]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To transform a feature to have a mean of 0 and a standard deviation of 1, \n",
    "# use scikit-learn’s StandardScaler to perform both transformations:\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "# Create feature\n",
    "x = np.array([[-1000.1],\n",
    " [-200.2],\n",
    " [500.5],\n",
    " [600.6],\n",
    " [9000.9]])\n",
    "# Create scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Transform the feature\n",
    "standardized = scaler.fit_transform(x)\n",
    "# Show feature\n",
    "standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a270611f-a274-465f-bdf5-61e2f0dd565e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0\n",
      "Standard deviation: 1.0\n"
     ]
    }
   ],
   "source": [
    "# We can see the effect of standardization by looking at the mean and standard devia‐\n",
    "# tion of our solution’s output:\n",
    "# Print mean and standard deviation\n",
    "print(\"Mean:\", round(standardized.mean()))\n",
    "print(\"Standard deviation:\", standardized.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980ff438-8f29-45a9-b1e1-267866eb171e",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- If our data has significant outliers, it can negatively impact our standardization by\n",
    "affecting the feature’s mean and variance. In this scenario, it is often helpful to instead\n",
    "rescale the feature using the median and quartile range. In scikit-learn, we do this\n",
    "using the RobustScaler method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "826129ee-3b31-4cc3-88c8-7eb4e4e79f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.87387612],\n",
       "       [-0.875     ],\n",
       "       [ 0.        ],\n",
       "       [ 0.125     ],\n",
       "       [10.61488511]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create scaler\n",
    "robust_scaler = preprocessing.RobustScaler()\n",
    "# Transform feature\n",
    "robust_scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee131819-61b5-498e-8b8d-8bedc909f30b",
   "metadata": {},
   "source": [
    "# Normalizing Observations\n",
    "> - Many rescaling methods (e.g., min-max scaling and standardization) operate on\n",
    "features; however, we can also rescale across individual observations.\n",
    "> - Normalizer rescales the values on individual observations to have unit norm (the sum of their\n",
    "lengths is 1).\n",
    ">> - This type of rescaling is often used when we have many equivalent\n",
    "features (e.g., text classification when every word or n-word group is a feature).\n",
    ">>> Normalizer provides three norm options\n",
    ">>> 1. Euclidean norm (often called L2) being the default argument:<br>\n",
    "`∥ x ∥2 = sqrt((x1)^2 + (x2)^2 + ⋯ + (xn)^2)`\n",
    ">>>>where x is an individual observation and xn is that observation’s value for the nth\n",
    "feature.\n",
    ">>> 2. Manhattan norm (L1):<br>\n",
    "`∥ x ∥1 = ∑(i = 1 till n)xi`\n",
    ">>>>Intuitively, L2 norm can be thought of as the distance between two points in New\n",
    "York for a bird (i.e., a straight line), while L1 can be thought of as the distance for a\n",
    "human walking on the street (walk north one block, east one block, north one block,\n",
    "east one block, etc.), which is why it is called `“Manhattan norm”` or `“Taxicab norm.”`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6319abe-fa75-4eec-92b6-a1866b763943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.70710678 0.70710678]\n",
      " [0.30782029 0.95144452]\n",
      " [0.07405353 0.99725427]\n",
      " [0.04733062 0.99887928]\n",
      " [0.95709822 0.28976368]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.70710678, 0.70710678],\n",
       "       [0.30782029, 0.95144452],\n",
       "       [0.07405353, 0.99725427],\n",
       "       [0.04733062, 0.99887928],\n",
       "       [0.95709822, 0.28976368]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To rescale the feature values of observations to have unit norm (a totallength of 1),\n",
    "# use Normalizer with a norm argument:\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "# Create feature matrix\n",
    "features = np.array([[0.5, 0.5],\n",
    " [1.1, 3.4],\n",
    " [1.5, 20.2],\n",
    " [1.63, 34.4],\n",
    " [10.9, 3.3]])\n",
    "# Create normalizer\n",
    "normalizer = Normalizer(norm=\"l2\")\n",
    "# Transform feature matrix\n",
    "print(normalizer.transform(features))\n",
    "# array([[ 0.70710678, 0.70710678],\n",
    "#  [ 0.30782029, 0.95144452],\n",
    "#  [ 0.07405353, 0.99725427],\n",
    "#  [ 0.04733062, 0.99887928],\n",
    "#  [ 0.95709822, 0.28976368]])\n",
    "# ----- or -----------------\n",
    "# Transform feature matrix\n",
    "features_l2_norm = Normalizer(norm=\"l2\").transform(features)\n",
    "# Show feature matrix\n",
    "features_l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0e13102-23f6-4e66-81e2-24ad9a6a08d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       ],\n",
       "       [0.24444444, 0.75555556],\n",
       "       [0.06912442, 0.93087558],\n",
       "       [0.04524008, 0.95475992],\n",
       "       [0.76760563, 0.23239437]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform feature matrix\n",
    "features_l1_norm = Normalizer(norm=\"l1\").transform(features)\n",
    "# Show feature matrix\n",
    "features_l1_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f153833-c8e2-4a76-ab52-19b7bc57e911",
   "metadata": {},
   "source": [
    "- Practically, notice that `norm=\"l1\"` rescales an observation’s values so they sum to 1,\n",
    "which can sometimes be a desirable quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af7079c4-9699-4a24-9219-c73c2f4423cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of the first observation's values: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Print sum\n",
    "print(\"Sum of the first observation\\'s values:\",\n",
    " features_l1_norm[0, 0] + features_l1_norm[0, 1])\n",
    "\n",
    "# Note: Sum of the first observation's values: 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e75fc3-826d-4559-bb73-16fcd9d69a9f",
   "metadata": {},
   "source": [
    "# Generating Polynomial and Interaction Features\n",
    "> - Polynomial features are often created when we want to include the notion that there\n",
    "exists a nonlinear relationship between the features and the target. For example, we\n",
    "might suspect that the effect of age on the probability of having a major medical\n",
    "condition is not constant over time but increases as age increases. We can encode that\n",
    "nonconstant effect in a feature, x, by generating that feature’s higher-order forms (x^2, x^3, etc.).\n",
    "\n",
    "> **Note:**\n",
    "> > Often we run into situations where the effect of one feature is dependent\n",
    "on another feature.\n",
    "> > > A simple example would be if we were trying to predict whether\n",
    "or not our coffee was sweet, and we had two features: (1) whether or not the coffee\n",
    "was stirred, and (2) whether or not we added sugar.<br><br>\n",
    "> > > Individually, each feature does\n",
    "not predict coffee sweetness, but the combination of their effects does. That is, a\n",
    "coffee would only be sweet if the coffee had sugar and was stirred.\n",
    "> > > - The effects of each\n",
    "feature on the target (sweetness) are dependent on each other.\n",
    ">>> -  We can encode that\n",
    "relationship by including an interaction feature that is the product of the individual\n",
    "features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36453972-9e46-4252-94c3-8d43c578f5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 3., 4., 6., 9.],\n",
       "       [2., 3., 4., 6., 9.],\n",
       "       [2., 3., 4., 6., 9.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To create polynomial and interaction features.\n",
    "# use scikit-learn built-in method:\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Create feature matrix\n",
    "features = np.array([[2, 3],\n",
    " [2, 3],\n",
    " [2, 3]])\n",
    "# Create PolynomialFeatures object\n",
    "polynomial_interaction = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# Create polynomial features\n",
    "polynomial_interaction.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a61f5b-154c-40ed-a59c-8c0bd620095f",
   "metadata": {},
   "source": [
    "- The degree parameter determines the maximum degree of the polynomial. For\n",
    "example, degree=2 will create new features raised to the second power:<br>\n",
    "`x1, x2, (x1)^2, (x1)^2, (x2)^2`\n",
    "<br>\n",
    "- while degree=3 will create new features raised to the second and third power:<br>\n",
    "`x1, x2, (x1)^2, (x2)^2,(x1)^3, (x2)^3,(x1)^2, (x1)^3, (x2)^3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd62c0d2-fb86-49c9-837c-8331b3933429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 3., 6.],\n",
       "       [2., 3., 6.],\n",
       "       [2., 3., 6.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Furthermore, by default PolynomialFeatures includes interaction features,\n",
    "# We can restrict the features created to only interaction features by setting\n",
    "# interaction_only to True:\n",
    "interaction = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "interaction.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bac75d9-59de-42fa-8adc-9284e597646b",
   "metadata": {},
   "source": [
    "# Transforming Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93ed96bc-6cd9-48b2-bd10-431391e38395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 13],\n",
       "       [12, 13],\n",
       "       [12, 13]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To make a custom transformation to one or more features.\n",
    "# use FunctionTransformer in scikit-learn to apply a function to a set of features:\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "# Create feature matrix\n",
    "features = np.array([[2, 3],\n",
    " [2, 3],\n",
    " [2, 3]])\n",
    "# Define a simple function\n",
    "def add_ten(x: int) -> int:\n",
    "    return x + 10\n",
    "# Create transformer\n",
    "ten_transformer = FunctionTransformer(add_ten)\n",
    "# Transform feature matrix\n",
    "ten_transformer.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3beaafe3-9b0c-475e-8b2f-554521d03806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2\n",
       "0         12         13\n",
       "1         12         13\n",
       "2         12         13"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can create the same transformation in pandas using apply:\n",
    "# Load library\n",
    "import pandas as pd\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(features, columns=[\"feature_1\", \"feature_2\"])\n",
    "# Apply function\n",
    "df.apply(add_ten)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa6bed0-1826-47bf-a34c-2596568295a4",
   "metadata": {},
   "source": [
    "# Detecting Outliers\n",
    "> - There is no single best technique for detecting outliers.\n",
    "> - Instead, we have a collection\n",
    "of techniques all with their own advantages and disadvantages.\n",
    ">>  Our best strategy\n",
    "is often trying multiple techniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7880a70e-ec2b-415c-adcd-81871bd1e61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1,  1,  1,  1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To identify extreme observations, you have to detect outliers and a common\n",
    "# method is to assume the data is normally distributed and, based on that assumption,\n",
    "# “draw” an ellipse around the data, classifying any observation inside the ellipse as\n",
    "# an inlier (labeled as 1) and any observation outside the ellipse as an outlier (labeled as -1):\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.datasets import make_blobs\n",
    "# Create simulated data\n",
    "features, _ = make_blobs(n_samples = 10,\n",
    " n_features = 2,\n",
    " centers = 1,\n",
    " random_state = 1)\n",
    "# Replace the first observation's values with extreme values\n",
    "features[0,0] = 10000\n",
    "features[0,1] = 10000\n",
    "# Create detector\n",
    "outlier_detector = EllipticEnvelope(contamination=.1)\n",
    "# Fit detector\n",
    "outlier_detector.fit(features)\n",
    "# Predict outliers\n",
    "outlier_detector.predict(features)\n",
    "# array([-1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c4cda-ed55-4015-9dc1-696fa1552d30",
   "metadata": {},
   "source": [
    "> - In these arrays, values of -1 refer to outliers whereas values of 1 refer to inliers.\n",
    "> - A major limitation of this approach is the need to specify a contamination parameter,\n",
    "which is the proportion of observations that are outliers—a value that we don’t\n",
    "know.\n",
    ">> - Think of contamination as our estimate of the cleanliness of our data. If we\n",
    "expect our data to have few outliers, we can set contamination to something small.\n",
    "However, if we believe that the data is likely to have outliers, we can set it to a higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ca4408-8517-4652-954e-c0cee2d75b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0], dtype=int64),)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instead of looking at observations as a whole, we can instead look at individual\n",
    "# features and identify extreme values in those features using interquartile range (IQR):\n",
    "# Create one feature\n",
    "feature = features[:,0]\n",
    "# Create a function to return index of outliers\n",
    "def indicies_of_outliers(x: int) -> np.array(int):\n",
    "    q1, q3 = np.percentile(x, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - (iqr * 1.5)\n",
    "    upper_bound = q3 + (iqr * 1.5)\n",
    "    return np.where((x > upper_bound) | (x < lower_bound))\n",
    "# Run function\n",
    "indicies_of_outliers(feature)\n",
    "# (array([0]),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b7a52d-9c5b-413b-ab80-d4a011ed76dc",
   "metadata": {},
   "source": [
    "> - IQR is the difference between the first and third quartile of a set of data. You can\n",
    "think of IQR as the spread of the bulk of the data, with outliers being observations far\n",
    "from the main concentration of data.\n",
    ">> - Outliers are commonly defined as any value 1.5\n",
    "IQRs less than the first quartile, or 1.5 IQRs greater than the third quartile value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec4449-681c-4cd7-8890-4f66b2357bd9",
   "metadata": {},
   "source": [
    "# Handling Outliers\n",
    "> - If we believe they are errors in the data, such as from a broken\n",
    "sensor or a miscoded value, then we might drop the observation or replace outlier\n",
    "values with NaN since we can’t trust those values. \n",
    "> - Second, how we handle outliers should be based on our goal for machine learning.\n",
    ">> For example, if we want to predict house prices based on features of the house, we\n",
    "might reasonably assume the price for mansions with over 100 bathrooms is driven\n",
    "by a different dynamic than regular family homes.\n",
    ">_____________________________________________________________\n",
    "\n",
    "> what should we do if we have outliers?\n",
    "> > Think about why they are outliers, have an\n",
    "end goal in mind for the data, and, most importantly, remember that not making a\n",
    "decision to address outliers is itself a decision with implications.\n",
    ">>> - **Note:** if you do have outliers, standardization might not be appropriate\n",
    "because the mean and variance might be highly influenced by the outliers. In this\n",
    "case, use a rescaling method more robust against outliers, like `RobustScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bd2bb26-ba97-4579-9675-d543ddb5258b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Bathrooms</th>\n",
       "      <th>Square_Feet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>534433</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>392333</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>293222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Price  Bathrooms  Square_Feet\n",
       "0  534433        2.0         1500\n",
       "1  392333        3.5         2500\n",
       "2  293222        2.0         1500"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have outliers in your data that you want to identify and then reduce their impacton the data distribution.\n",
    "# Typically we can use three strategies to handle outliers. \n",
    "# First, we can drop them:\n",
    "# Load library\n",
    "import pandas as pd\n",
    "# Create DataFrame\n",
    "houses = pd.DataFrame()\n",
    "houses['Price'] = [534433, 392333, 293222, 4322032]\n",
    "houses['Bathrooms'] = [2, 3.5, 2, 116]\n",
    "houses['Square_Feet'] = [1500, 2500, 1500, 48000]\n",
    "# Filter observations\n",
    "houses[houses['Bathrooms'] < 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c70eec0-7a4e-4d28-b49c-72c90da7fb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Bathrooms</th>\n",
       "      <th>Square_Feet</th>\n",
       "      <th>Outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>534433</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>392333</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>293222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4322032</td>\n",
       "      <td>116.0</td>\n",
       "      <td>48000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Price  Bathrooms  Square_Feet  Outlier\n",
       "0   534433        2.0         1500        0\n",
       "1   392333        3.5         2500        0\n",
       "2   293222        2.0         1500        0\n",
       "3  4322032      116.0        48000        1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second, we can mark them as outliers and include “Outlier” as a feature:\n",
    "# Load library\n",
    "import numpy as np\n",
    "# Create feature based on boolean condition\n",
    "houses[\"Outlier\"] = np.where(houses[\"Bathrooms\"] < 20, 0, 1)\n",
    "# Show data\n",
    "houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3d7056b-b9d9-42ba-a6c2-99be43934710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Bathrooms</th>\n",
       "      <th>Square_Feet</th>\n",
       "      <th>Outlier</th>\n",
       "      <th>Log_Of_Square_Feet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>534433</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>392333</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>293222</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4322032</td>\n",
       "      <td>116.0</td>\n",
       "      <td>48000</td>\n",
       "      <td>1</td>\n",
       "      <td>10.778956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Price  Bathrooms  Square_Feet  Outlier  Log_Of_Square_Feet\n",
       "0   534433        2.0         1500        0            7.313220\n",
       "1   392333        3.5         2500        0            7.824046\n",
       "2   293222        2.0         1500        0            7.313220\n",
       "3  4322032      116.0        48000        1           10.778956"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, we can transform the feature to dampen the effect of the outlier:\n",
    "# Log feature\n",
    "houses[\"Log_Of_Square_Feet\"] = [np.log(x) for x in houses[\"Square_Feet\"]]\n",
    "# Show data\n",
    "houses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e93642-16ef-4f19-b297-e2c273d71879",
   "metadata": {},
   "source": [
    "# Discretizating Features\n",
    "> - Discretization can be a fruitful strategy when we have reason to believe that a numerical feature should behave more like a categorical feature.\n",
    "> - **Note:** we can use clustering as a preprocessing step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45eefd88-662b-40ee-9667-3c3f2f629821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have a numerical feature and want to break it up into discrete bins.\n",
    "# Depending on how we want to break up the data, there are two techniques we can use, \n",
    "# First, we can binarize the feature according to some threshold:\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Binarizer\n",
    "# Create feature\n",
    "age = np.array([[6],\n",
    " [12],\n",
    " [20],\n",
    " [36],\n",
    " [65]])\n",
    "# Create binarizer\n",
    "binarizer = Binarizer(threshold=18)\n",
    "# Transform feature\n",
    "binarizer.fit_transform(age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfd3578f-8c9b-4ddb-b0d4-7c616d1dd990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second, we can break up numerical features according to multiple thresholds:\n",
    "# Bin feature\n",
    "np.digitize(age, bins=[20,30,64])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe712c5-11c8-45f4-b6fc-7f933eb0e12c",
   "metadata": {},
   "source": [
    "# Deleting Observations with Missing Values\n",
    "> - Most machine learning algorithms cannot handle any missing values in the target and\n",
    "feature arrays. The simplest solution is to delete every observation that contains one or more\n",
    "missing values.\n",
    ">> That said, we should be very reluctant to delete observations with missing values.\n",
    "Deleting them is the nuclear option\n",
    "\n",
    "> There are three types of missing data:\n",
    "\n",
    "> 1. Missing completely at random (MCAR):\n",
    ">> The probability that a value is missing is independent of everything.<br><br>\n",
    ">> For example,\n",
    "a survey respondent rolls a die before answering a question: if she rolls a six, she\n",
    "skips that question.\n",
    ">_________________________________________\n",
    "> 2. Missing at random (MAR):\n",
    ">> The probability that a value is missing is not completely random but depends\n",
    "on the information captured in other features.<br><br>\n",
    ">> For example, a survey asks about\n",
    "gender identity and annual salary, and women are more likely to skip the salary\n",
    "question; however, their nonresponse depends only on information we have\n",
    "captured in our gender identity feature.\n",
    ">____________________________________\n",
    "> 3. Missing not at random (MNAR):\n",
    ">> The probability that a value is missing is not random and depends on informa‐\n",
    "tion not captured in our features.<br><br>\n",
    ">> For example, a survey asks about annual salary,\n",
    "and women are more likely to skip the salary question, and we do not have a\n",
    "gender identity feature in our data.<br>\n",
    "\n",
    "> - **Note:** It is sometimes acceptable to delete observations if they are MCAR or MAR.\n",
    "However, if the value is MNAR, the fact that a value is missing is itself information.\n",
    "Deleting MNAR observations can inject bias into our data because we are removing\n",
    "observations produced by some unobserved systematic effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "298a9e5a-3600-482b-bbaf-eb7f440ecbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.1, 11.1],\n",
       "       [ 2.2, 22.2],\n",
       "       [ 3.3, 33.3],\n",
       "       [ 4.4, 44.4]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you need to delete observations containing missing values, is easy with a clever line of NumPy:\n",
    "# Load library\n",
    "import numpy as np\n",
    "# Create feature matrix\n",
    "features = np.array([[1.1, 11.1],\n",
    " [2.2, 22.2],\n",
    " [3.3, 33.3],\n",
    " [4.4, 44.4],\n",
    " [np.nan, 55]])\n",
    "# Keep only observations that are not (denoted by ~) missing\n",
    "features[~np.isnan(features).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49fea1b3-8174-4d61-8e85-a8138460428b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.2</td>\n",
       "      <td>22.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.3</td>\n",
       "      <td>33.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.4</td>\n",
       "      <td>44.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2\n",
       "0        1.1       11.1\n",
       "1        2.2       22.2\n",
       "2        3.3       33.3\n",
       "3        4.4       44.4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternatively, we can drop missing observations using pandas:\n",
    "# Load library\n",
    "import pandas as pd\n",
    "# Load data\n",
    "dataframe = pd.DataFrame(features, columns=[\"feature_1\", \"feature_2\"])\n",
    "# Remove observations with missing values\n",
    "dataframe.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d89a3b5-d5b9-4d84-90ac-f0eda27e1c3e",
   "metadata": {},
   "source": [
    "# Imputing Missing Values\n",
    "> There are two main strategies for replacing missing data with substitute :<br>\n",
    ">> **First:** we can use machine learning to\n",
    "predict the values of the missing data. To do this we treat the feature with missing\n",
    "values as a target vector and use the remaining subset of features to predict missing\n",
    "values.<br>\n",
    ">>> A popular choice is KNN, the\n",
    "short explanation is that the algorithm uses the k nearest observations (according to\n",
    "some distance metric) to predict the missing value. In the below code we predicted the\n",
    "missing value using the five closest observations.<br>\n",
    ">>> - **Note:** The downside to KNN is that in order to know which observations are the closest to\n",
    "the missing value, it needs to calculate the distance between the missing value and\n",
    "every single observation. This is reasonable in smaller datasets but quickly becomes\n",
    "problematic if a dataset has millions of observations. In such cases, approximate\n",
    "nearest neighbors (ANN) is a more feasible approach.<br>\n",
    ">>______________________________________________\n",
    ">> **Second:** we can use scikit-learn’s SimpleImputer class from the imputer module\n",
    "to fill in missing values with the feature’s mean, median, or most frequent value.\n",
    ">>> - We will typically get worse results than with KNN ;(\n",
    "<br><br>\n",
    "> - **Note:** If we use imputation, it is a good idea to create a binary feature indicating whether\n",
    "the observation contains an imputed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c22d7608-792d-4d63-bf8c-76b777103710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Value: 0.8730186113995938\n",
      "Imputed Value: 1.0959262913919632\n"
     ]
    }
   ],
   "source": [
    "# If you have missing values in your data and want to impute them via a generic methodor prediction.\n",
    "# You can impute missing values using k-nearest neighbors (KNN) or the scikit-learn SimpleImputer class. \n",
    "# If you have a small amount of data, predict and impute the missing values using k-nearest neighbors:\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "# Make a simulated feature matrix\n",
    "features, _ = make_blobs(n_samples = 1000,\n",
    " n_features = 2,\n",
    " random_state = 1)\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "standardized_features = scaler.fit_transform(features)\n",
    "\n",
    "# Replace the first feature's first value with a missing value\n",
    "true_value = standardized_features[0,0]\n",
    "standardized_features[0,0] = np.nan\n",
    "# Predict the missing values in the feature matrix\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "features_knn_imputed = knn_imputer.fit_transform(standardized_features)\n",
    "# Compare true and imputed values\n",
    "print(\"True Value:\", true_value)\n",
    "print(\"Imputed Value:\", features_knn_imputed[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b4f4de5-bad1-447e-8e09-3d18e4d501c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Value: 0.8730186113995938\n",
      "Imputed Value: -3.058372724614996\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, we can use scikit-learn’s SimpleImputer class from the imputer module\n",
    "# to fill in missing values with the feature’s mean, median, or most frequent value.\n",
    "# However, we will typically get worse results than with KNN:\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "# Make a simulated feature matrix\n",
    "features, _ = make_blobs(n_samples = 1000,\n",
    " n_features = 2,\n",
    " random_state = 1)\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "standardized_features = scaler.fit_transform(features)\n",
    "# Replace the first feature's first value with a missing value\n",
    "true_value = standardized_features[0,0]\n",
    "standardized_features[0,0] = np.nan\n",
    "# Create imputer using the \"mean\" strategy\n",
    "mean_imputer = SimpleImputer(strategy=\"mean\")\n",
    "# Impute values\n",
    "features_mean_imputed = mean_imputer.fit_transform(features)\n",
    "# Compare true and imputed values\n",
    "print(\"True Value:\", true_value)\n",
    "print(\"Imputed Value:\", features_mean_imputed[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96401b-fc8e-4516-befc-7dc1dd7f0dae",
   "metadata": {},
   "source": [
    "# END of Chapter 04 ---> Handling Numeric Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
