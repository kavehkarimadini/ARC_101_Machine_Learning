{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35996cb1-d07c-4f81-8c58-be6e75a67000",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "> - Unstructured text data, like the contents of a book or a tweet, is both one of the most\n",
    "interesting sources of features and one of the most complex to handle.\n",
    "> - For transforming text into information-rich features we use\n",
    "some out-of-the-box features (termed embeddings) that have become increasingly\n",
    "ubiquitous in tasks that involve natural language processing **(NLP)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a845089-438a-4e90-9478-afae6f05199e",
   "metadata": {},
   "source": [
    "# Cleaning Text\n",
    "> - Some text data will need to be cleaned before we can use it to build features, or\n",
    "be preprocessed in some way prior to being fed into an algorithm.\n",
    "> - Most basic text cleaning can be completed using Python’s standard string operations.\n",
    "> - In the real world, we will most likely define a custom cleaning function (e.g., capitalizer)\n",
    "combining some cleaning tasks and apply that to the text data.\n",
    ">> -  Although cleaning\n",
    "strings can remove some information, it makes the data much easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99eee1b1-9ead-4f11-92a2-b38401b62f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang By Aishwarya Henriette',\n",
       " 'Parking And Going By Karl Gautier',\n",
       " 'Today Is The night By Jarek Prakash']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have some unstructured text data and want to complete some basic cleaning.\n",
    "# In the following example, we look at the text for three books and clean it by using\n",
    "# Python’s core string operations, in particular strip, replace, and split:\n",
    "# Create text\n",
    "text_data = [\" Interrobang. By Aishwarya Henriette \",\n",
    " \"Parking And Going. By Karl Gautier\",\n",
    "\" Today Is The night. By Jarek Prakash \"]\n",
    "# Strip whitespaces\n",
    "strip_whitespace = [string.strip() for string in text_data]\n",
    "# Show text\n",
    "strip_whitespace\n",
    "\n",
    "# Remove periods\n",
    "remove_periods = [string.replace(\".\", \"\") for string in strip_whitespace]\n",
    "# Show text\n",
    "remove_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec25f706-b6e4-40cd-b4bf-764a08193374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INTERROBANG BY AISHWARYA HENRIETTE',\n",
       " 'PARKING AND GOING BY KARL GAUTIER',\n",
       " 'TODAY IS THE NIGHT BY JAREK PRAKASH']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We also create and apply a custom transformation function:\n",
    "# Create function\n",
    "def capitalizer(string: str) -> str:\n",
    " return string.upper()\n",
    "# Apply function\n",
    "[capitalizer(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98930adf-3b37-4038-9bc9-5b35526415c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX',\n",
       " 'XXXXXXX XXX XXXXX XX XXXX XXXXXXX',\n",
       " 'XXXXX XX XXX XXXXX XX XXXXX XXXXXXX']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, we can use regular expressions to make powerful string operations:\n",
    "# Import library\n",
    "import re\n",
    "# Create function\n",
    "def replace_letters_with_X(string: str) -> str:\n",
    " return re.sub(r\"[a-zA-Z]\", \"X\", string)\n",
    "# Apply function\n",
    "[replace_letters_with_X(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "742197da-6abf-48ef-a6e1-babdf85747d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "b'machine learning in python cookbook'\n",
      "machine learning in python cookbook\n"
     ]
    }
   ],
   "source": [
    "# Define a string\n",
    "s = \"machine learning in python cookbook\"\n",
    "# Find the first index of the letter \"n\"\n",
    "find_n = s.find(\"n\")\n",
    "print(find_n)\n",
    "# Whether or not the string starts with \"m\"\n",
    "starts_with_m = s.startswith(\"m\")\n",
    "print(starts_with_m)\n",
    "# Whether or not the string ends with \"python\"\n",
    "ends_with_python = s.endswith(\"python\")\n",
    "print(ends_with_python)\n",
    "# Is the string alphanumeric\n",
    "is_alnum = s.isalnum()\n",
    "print(is_alnum)\n",
    "# Is it composed of only alphabetical characters (not including spaces)\n",
    "is_alpha = s.isalpha()\n",
    "print(is_alpha)\n",
    "# Encode as utf-8\n",
    "encode_as_utf8 = s.encode(\"utf-8\")\n",
    "print(encode_as_utf8)\n",
    "# Decode the same utf-8\n",
    "decode = encode_as_utf8.decode(\"utf-8\")\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e043db-535f-49e8-91e4-f07bb202eeef",
   "metadata": {},
   "source": [
    "# Parsing and Cleaning HTML\n",
    "> - Despite the strange name, `Beautiful Soup` is a powerful Python library designed for\n",
    "scraping HTML.\n",
    ">> - Typically Beautiful Soup is used to process HTML during live web\n",
    "scraping, but we can just as easily use it to extract text data embedded in static\n",
    "HTML.\n",
    ">>> - The method bellow shows how easy it can be to parse HTML\n",
    "and extract information from specific tags using find()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba294b84-5c84-4f23-843a-94c1feba2ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Masego Azra'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have text data with HTML elements and want to extract just the text,\n",
    "# use Beautiful Soup’s extensive set of options to parse and extract from HTML:\n",
    "# Load library\n",
    "from bs4 import BeautifulSoup\n",
    "# Create some HTML code\n",
    "html = \"<div class='full_name'>\"\\\n",
    " \"<span style='font-weight:bold'>Masego\"\\\n",
    " \"</span> Azra</div>\"\n",
    "# Parse html\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "# Find the div with the class \"full_name\", show text\n",
    "soup.find(\"div\", { \"class\" : \"full_name\" }).text\n",
    "# 'Masego Azra'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d32ca5d-9e6d-49c8-af8c-766748eb931d",
   "metadata": {},
   "source": [
    "# Removing Punctuation\n",
    "> - The Python translate method is popular due to its speed.\n",
    ">> 1. First\n",
    "we created a dictionary, punctuation, with all punctuation characters according to\n",
    "Unicode as its keys and None as its values.\n",
    ">> 2. Next we translated all characters in the\n",
    "string that are in punctuation into None, effectively removing them. There are more\n",
    "readable ways to remove punctuation, but this somewhat hacky solution has the\n",
    "advantage of being far faster than alternatives.<br>\n",
    "\n",
    "> **Note:** It is important to be conscious of the fact that punctuation contains information (e.g.,\n",
    "“Right?” versus “Right!”). If the punctuation is important we should\n",
    "make sure to take that into account. Depending on the downstream task we’re trying\n",
    "to accomplish, punctuation might contain important information we want to keep\n",
    "(e.g., using a “?” to classify if some text contains a question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2123dd08-aa6b-45b5-a4ee-e997e6edb384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi I Love This Song', '10000 Agree LoveIT', 'Right']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have a feature of text data and want to remove punctuation.\n",
    "# Define a function that uses translate with a dictionary of punctuation characters:\n",
    "# Load libraries\n",
    "import unicodedata\n",
    "import sys\n",
    "# Create text\n",
    "text_data = ['Hi!!!! I. Love. This. Song....',\n",
    " '10000% Agree!!!! #LoveIT',\n",
    " 'Right?!?!']\n",
    "# Create a dictionary of punctuation characters\n",
    "punctuation = dict.fromkeys(\n",
    " (i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P')), None)\n",
    "# For each string, remove any punctuation characters\n",
    "[string.translate(punctuation) for string in text_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0737906-df5e-4ad2-95f5-8fef9b397142",
   "metadata": {},
   "source": [
    "# Tokenizing Text\n",
    "> Tokenization, especially word tokenization, is a common task after cleaning text data\n",
    "because it is the first step in the process of turning the text into data we will use\n",
    "to construct useful features.\n",
    "> > Some pretrained NLP models (such as `Google’s BERT`)\n",
    "utilize model-specific tokenization techniques; however, word-level tokenization is\n",
    "still a fairly common tokenization approach before getting features from individual\n",
    "words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95e83460-a30c-486a-8533-35d10ea26187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IF you have text and want to break it up into individual words.\n",
    "# Natural Language Toolkit for Python (NLTK) has a powerful set of text manipulation\n",
    "# operations, including word tokenizing:\n",
    "# Load library\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Create text\n",
    "string = \"The science of today is the technology of tomorrow\"\n",
    "# Tokenize words\n",
    "word_tokenize(string)\n",
    "# ['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ac80ee2-ac34-4a05-af76-ae3526d51cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The science of today is the technology of tomorrow.', 'Tomorrow is today.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also tokenize into sentences:\n",
    "# Load library\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# Create text\n",
    "string = \"The science of today is the technology of tomorrow. Tomorrow is today.\"\n",
    "# Tokenize sentences\n",
    "sent_tokenize(string)\n",
    "# ['The science of today is the technology of tomorrow.', 'Tomorrow is today.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d6819-8f46-477a-9fc1-c480aeebe310",
   "metadata": {},
   "source": [
    "# Removing Stop Words\n",
    "> While “stop words” can refer to any set of words we want to remove before processing, frequently the term refers to extremely common words that themselves contain\n",
    "little information value.\n",
    "> > Whether or not you choose to remove stop words will\n",
    "depend on your individual use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d75e4c3-c0c7-4e1b-beb3-5f4e9e3a16c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['going', 'go', 'store', 'park']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given tokenized text data, if you want to remove extremely common words (e.g., a, is, of, on)\n",
    "# that contain little informational value, use NLTK’s stopwords:\n",
    "# Load library\n",
    "from nltk.corpus import stopwords\n",
    "# You will have to download the set of stop words the first time\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# Create word tokens\n",
    "tokenized_words = ['i',\n",
    " 'am',\n",
    " 'going',\n",
    " 'to',\n",
    " 'go',\n",
    " 'to',\n",
    " 'the',\n",
    " 'store',\n",
    " 'and',\n",
    " 'park']\n",
    "# Load stop words\n",
    "stop_words = stopwords.words('english')\n",
    "# Remove stop words\n",
    "[word for word in tokenized_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1b11675-362e-4d82-87b5-fce21cf2c828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK has a list of common stop words that we can use to find and remove stop words in our tokenized words:\n",
    "# Show stop words\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0775c82e-bfff-40f1-881f-31ca9056df07",
   "metadata": {},
   "source": [
    "# Stemming Words\n",
    "> Stemming reduces a word to its stem by identifying and removing affixes (e.g., ger‐\n",
    "unds) while keeping the root meaning of the word. For example, both “tradition” and\n",
    "“traditional” have “tradit” as their stem, indicating that while they are different words,\n",
    "they represent the same general concept.\n",
    "> >By stemming our text data, we transform\n",
    "it to something less readable but closer to its base meaning and thus more suitable\n",
    "for comparison across observations.\n",
    "> >> - NLTK’s `PorterStemmer` implements the widely\n",
    "used Porter stemming algorithm to remove or replace common suffixes to produce\n",
    "the word stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a934322c-34a0-4ad4-abd1-a67c78a521dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have tokenized words and want to convert them into their root forms, use NLTK’s PorterStemmer:\n",
    "# Load library\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# Create word tokens\n",
    "tokenized_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']\n",
    "# Create stemmer\n",
    "porter = PorterStemmer()\n",
    "# Apply stemmer\n",
    "[porter.stem(word) for word in tokenized_words]\n",
    "# ['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09afaea-7d7e-4f4c-bb4d-1ff156d01755",
   "metadata": {},
   "source": [
    "# Tagging Parts of Speech\n",
    "> If our text is English and not on a specialized topic (e.g., medicine) the simplest\n",
    "solution is to use NLTK’s pretrained parts-of-speech tagger.\n",
    "> - However, if pos_tag is not very accurate, NLTK also gives us the ability to train our own tagger.\n",
    ">> - The major\n",
    "downside of training a tagger is that we need a large corpus of text where the tag of\n",
    "each word is known.\n",
    ">>> Constructing this tagged corpus is obviously labor intensive and\n",
    "is probably going to be a last resort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c8ffd51-e559-4ab8-8c25-adef5da64456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kaveh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d8adc14-1696-46bf-814a-3289ebd304bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have text data and want to tag each word or character with its part of speech,\n",
    "# use NLTK’s pretrained parts-of-speech tagger:\n",
    "# Load libraries\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "# Create text\n",
    "text_data = \"Chris loved outdoor running\"\n",
    "# Use pretrained part of speech tagger\n",
    "text_tagged = pos_tag(word_tokenize(text_data))\n",
    "# Show parts of speech\n",
    "text_tagged\n",
    "# [('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c2c04-810a-4d8e-bf10-f993dd48c670",
   "metadata": {},
   "source": [
    "> - The output is a list of tuples with the word and the tag of the part of speech.\n",
    "> - NLTK uses the Penn Treebank parts for speech tags.\n",
    ">> - Some examples of the Penn Treebank tags are:\n",
    "\n",
    ">> |Tag | Part of speech                    |\n",
    "| ----------- | ----------- |\n",
    "|NNP | Proper noun, singular             |\n",
    "|NN  | Noun, singular or mass            |\n",
    "|RB  | Adverb                            |\n",
    "|VBD | Verb, past tense                  |\n",
    "|VBG | Verb, gerund or present participle|\n",
    "|JJ  | Adjective                         |\n",
    "|PRP | Personal pronoun                  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14581d67-9ef7-400b-9039-865b73c51201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chris']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Once the text has been tagged, we can use the tags to find certain parts of speech. \n",
    "# For example, here are all nouns:\n",
    "# Filter words\n",
    "[word for word, tag in text_tagged if tag in ['NN','NNS','NNP','NNPS'] ]\n",
    "# ['Chris']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4d2f075-6f39-4948-981d-56275d33e29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A more realistic situation would be to have data where every observation contains\n",
    "# a tweet, and we want to convert those sentences into features for individual parts of\n",
    "# speech (e.g., a feature with 1 if a proper noun is present, and 0 otherwise):\n",
    "\n",
    "# Import libraries\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# Create text\n",
    "tweets = [\"I am eating a burrito for breakfast\",\n",
    " \"Political science is an amazing field\",\n",
    " \"San Francisco is an awesome city\"]\n",
    "# Create list\n",
    "tagged_tweets = []\n",
    "# Tag each word and each tweet\n",
    "for tweet in tweets:\n",
    " tweet_tag = nltk.pos_tag(word_tokenize(tweet))\n",
    " tagged_tweets.append([tag for word, tag in tweet_tag])\n",
    "\n",
    "# Use one-hot encoding to convert the tags into features\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "one_hot_multi.fit_transform(tagged_tweets)\n",
    "# array([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
    "#  [1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
    "#  [1, 0, 1, 1, 1, 0, 0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bcabf71-a779-4e58-8ac1-ea617fdb7a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using classes_ we can see that each feature is a part-of-speech tag:\n",
    "# Show feature names\n",
    "one_hot_multi.classes_\n",
    "# array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'], dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f125a4-cccc-4a05-85e0-e02ae3b15169",
   "metadata": {},
   "source": [
    "# Performing Named-Entity Recognition\n",
    "> Named-entity recognition is the process of recognizing specific entities from text.\n",
    ">> Tools like `spaCy` offer preconfigured pipelines, and even pretrained or fine-tuned\n",
    "machine learning models that can easily identify these entities.\n",
    ">>> In this case, we\n",
    "use spaCy to identify a person (“Elon Musk”), organization (“Twitter”), and money\n",
    "value (“21B”) from the raw text.\n",
    ">>> - Using this information, we can extract structured\n",
    "information from the unstructured textual data. This information can then be used in\n",
    "downstream machine learning models or data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc04c1f0-c48a-41e2-b874-de18f63969d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF you want to perform named-entity recognition in freeform text (such as “Person,”“State,” etc.).\n",
    "# use spaCy’s default named-entity recognition pipeline and models to extract entites from text:\n",
    "# Import libraries\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1104bbed-556c-4b49-8e49-9e941ea0c16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Elon Musk, Twitter, 21B)\n",
      "Elon Musk,PERSON\n",
      "Twitter,ORG\n",
      "21B,MONEY\n"
     ]
    }
   ],
   "source": [
    "spacy.prefer_gpu()\n",
    "# Load the spaCy package and use it to parse the text\n",
    "# make sure you have run \"python -m spacy download en\"\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "doc = nlp(\"Elon Musk offered to buy Twitter using $21B of his own money.\")\n",
    "# Print each entity\n",
    "print(doc.ents)\n",
    "# For each entity print the text and the entity label\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_, sep=\",\")\n",
    "# (Elon Musk, Twitter, 21B)\n",
    "# Elon Musk, PERSON\n",
    "# Twitter, ORG\n",
    "# 21B, MONEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6bfb37-c70b-44c0-bcf6-1ba03595fca3",
   "metadata": {},
   "source": [
    "# Encoding Text as a Bag of Words\n",
    "> - One of the most common methods of transforming text into features is using a\n",
    "`bag-of-words` model.\n",
    ">> - Bag-of-words models output a feature for every unique word\n",
    "in text data, with each feature containing a count of occurrences in observations.\n",
    ">>> For example, in the following code examples, the sentence “I love Brazil. Brazil!” has a value of 2 in the\n",
    "“brazil” feature because the word brazil appears two times.\n",
    ">_______________________________________________\n",
    "\n",
    "> Most words likely do not occur in most observations, and therefore bag-of-words\n",
    "feature matrices will contain mostly 0s as values called sparse:\n",
    "> > - Instead of storing all values of the matrix, we can store only nonzero values\n",
    "and then assume all other values are 0.\n",
    ">>- This will save memory when we have large\n",
    "feature matrices.\n",
    ">>> One of the nice features of `CountVectorizer` is that the output is a\n",
    "sparse matrix by `default`.\n",
    "> ____________________________________________________\n",
    "\n",
    "> `CountVectorizer` comes with a number of useful parameters to make it easy to create\n",
    "bag-of-words feature matrices.\n",
    "> 1. while by default every feature is a word, that\n",
    "does not have to be the case. Instead we can set every feature to be the combination\n",
    "of two words (called a `2-gram`) or even three words (`3-gram`).\n",
    ">> `ngram_range` sets the\n",
    "minimum and maximum size of our n-grams. For example, `(2,3) will return all\n",
    "2-grams and 3-grams`.\n",
    ">3. we can easily remove low-information filler words by\n",
    "using `stop_words`, either with a `built-in list or a custom list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04ba98cd-fb85-4aaf-90b7-a056d6fc2a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you have text data and want to create a set of features \n",
    "# indicating the number of times an observation’s text contains a particular word,\n",
    "# use scikit-learn’s CountVectorizer:\n",
    "# Load library\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Create text\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    " 'Sweden is best',\n",
    "'Germany beats both'])\n",
    "# Create the bag of words feature matrix\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "# Show feature matrix\n",
    "bag_of_words\n",
    "# <3x8 sparse matrix of type '<class 'numpy.int64'>'\n",
    "#  with 8 stored elements in Compressed Sparse Row format>\n",
    "\n",
    "# This output is a sparse array, which is often necessary when we have a large amount of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51fd919f-13f6-4cc3-b797-d163b7905050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# However, in our toy example we can use toarray to view a matrix of word\n",
    "# counts for each observation:\n",
    "bag_of_words.toarray()\n",
    "# array([[0, 0, 0, 2, 0, 0, 1, 0],\n",
    "#  [0, 1, 0, 0, 0, 1, 0, 1],\n",
    "#  [1, 0, 1, 0, 1, 0, 0, 0]], dtype=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bf522a5-9a98-4186-a2b0-05e43bba8939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love',\n",
       "       'sweden'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use the get_feature_names method to view the word associated with each feature:\n",
    "# Show feature names\n",
    "count.get_feature_names_out()\n",
    "# array(['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love',\n",
    "#  'sweden'], dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa69b0b-dc78-46c2-b79b-649ad85075f9",
   "metadata": {},
   "source": [
    "- **Note:** that the I from I love Brazil is not considered a token because the default\n",
    "token_pattern only considers tokens of two or more alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f1bfa53-6f98-4fbf-966c-13703f5530bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, we can restrict the words or phrases we want to consider to a certain list of words using vocabulary\n",
    "# For example, we could create a bag-of-words feature matrix only for occurrences of country names:\n",
    "# Create feature matrix with arguments\n",
    "count_2gram = CountVectorizer(ngram_range=(1,2), stop_words=\"english\", vocabulary=['brazil'])\n",
    "bag = count_2gram.fit_transform(text_data)\n",
    "# View feature matrix\n",
    "bag.toarray()\n",
    "# array([[2],\n",
    "#  [0],\n",
    "#  [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12d7fb43-a1c6-4973-a618-f55b1b1633b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brazil': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the 1-grams and 2-grams\n",
    "count_2gram.vocabulary_\n",
    "# {'brazil': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5c3581-e0ac-4caa-a35c-5298f85ac9f7",
   "metadata": {},
   "source": [
    "# Weighting Word Importance\n",
    "> - The more a word appears in a document, the more likely it is that the word is\n",
    "important to that document.\n",
    ">> - If the word economy appears frequently, it\n",
    "is evidence that the document might be about economics. We call this `term frequency (tf)`.\n",
    ">___________________________\n",
    "> - In contrast, if a word appears in many documents, it is likely less important to any\n",
    "individual document.\n",
    ">> - If every document in some text data contains the\n",
    "word after then it is probably an unimportant word. We call this `document frequency\n",
    "(df)`.\n",
    ">___________________________________________________\n",
    "> By combining these two statistics, we can assign a score to every word representing\n",
    "how important that word is in a document.\n",
    ">> - Specifically, we multiply tf to the inverse\n",
    "of document frequency (idf):<br>\n",
    "`tf‐idf(t, d) = tf(t, d) × idf(t)`\n",
    ">>> where t is a word (term) and d is a document. There are a number of variations in\n",
    "how tf and idf are calculated. In scikit-learn, `tf` is simply the number of times a word\n",
    "appears in the document, and `idf` is calculated as:<br>\n",
    "`idf(t) = log((1 + n|d)/(1 + df(d,t)) + 1`\n",
    ">>>> where `n|d` is the number of documents, and df d,t is term t’s document frequency\n",
    "(i.e., the number of documents where the term appears).\n",
    "\n",
    ">> - **Note:** By default, scikit-learn then normalizes the tf‐idf vectors using the Euclidean norm\n",
    "(L2 norm). The higher the resulting value, the more important the word is to a\n",
    "document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dafd28ab-e313-469c-97ac-9247556a9f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want a bag of words with words weighted by their importance to an observation.\n",
    "# Compare the frequency of the word in a document (a tweet, movie review, speech\n",
    "# transcript, etc.) with the frequency of the word in all other documents using term\n",
    "# frequency-inverse document frequency (tf‐idf). \n",
    "# scikit-learn makes this easy with TfidfVectorizer:\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create text\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    " 'Sweden is best',\n",
    "'Germany beats both'])\n",
    "# Create the tf-idf feature matrix\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "# Show tf-idf feature matrix\n",
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f5c401c-a823-4e75-bbd1-9f6a43e7ce5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.89442719, 0.        ,\n",
       "        0.        , 0.4472136 , 0.        ],\n",
       "       [0.        , 0.57735027, 0.        , 0.        , 0.        ,\n",
       "        0.57735027, 0.        , 0.57735027],\n",
       "       [0.57735027, 0.        , 0.57735027, 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the output is a sparse matrix. If we want to view the\n",
    "# output as a dense matrix, we can use toarray:\n",
    "# Show tf-idf feature matrix as dense matrix\n",
    "feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b6fbbb8-2339-4387-976c-40a9f3184b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 6,\n",
       " 'brazil': 3,\n",
       " 'sweden': 7,\n",
       " 'is': 5,\n",
       " 'best': 1,\n",
       " 'germany': 4,\n",
       " 'beats': 0,\n",
       " 'both': 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary_ shows us the word of each feature:\n",
    "# Show feature names\n",
    "tfidf.vocabulary_\n",
    "# {'love': 6,\n",
    "#  'brazil': 3,\n",
    "#  'sweden': 7,\n",
    "#  'is': 5,\n",
    "#  'best': 1,\n",
    "#  'germany': 4,\n",
    "#  'beats': 0,\n",
    "#  'both': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38d834e-b65d-4002-80c2-929300c58710",
   "metadata": {},
   "source": [
    "# Using Text Vectors to Calculate Text Similarity in a Search Query\n",
    "> Text vectors are incredibly useful for NLP use cases such as search engines.\n",
    "> > After\n",
    "calculating the `tf‐idf vectors` of a set of sentences or documents, we can use the same\n",
    "tfidf object to vectorize future sets of text.<br>\n",
    "> > Then, we can compute cosine similarity\n",
    "between our input vector and the matrix of other vectors and sort by the most\n",
    "relevant documents.\n",
    ">_________________________________\n",
    "> `Cosine similarities` take on the range of `[0, 1.0]`, with `0 being least similar and 1 being\n",
    "most similar`.\n",
    "> > Since we’re using tf‐idf vectors to compute the similarity between vectors,\n",
    "the frequency of a word’s occurrence is also taken into account.\n",
    "> > > - However, with a small\n",
    "corpus (set of documents) even “frequent” words may not appear frequently. In this\n",
    "example, “Sweden is best” is the most relevant text to our search query “Brazil is the\n",
    "best”.\n",
    ">>> - Since the query mentions Brazil, we might expect “I love Brazil. Brazil!” to be\n",
    "the most relevant; however, “Sweden is best” is the most similar due to the words “is”\n",
    "and “best”.\n",
    ">> - **Note:** As the number of documents we add to our corpus increases, less important\n",
    "words will be weighted less and have less effect on our cosine similarity calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75ef7c8f-7c3c-4a06-8f3d-efe65de75000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sweden is best', 0.6666666666666666), ('I love Brazil. Brazil!', 0.5163977794943222), ('Germany beats both', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "# If you want to use tf‐idf vectors to implement a text search function in Python.\n",
    "# Calculate the cosine similarity between tf‐idf vectors using scikit-learn:\n",
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "# Create searchable text data\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    " 'Sweden is best',\n",
    "'Germany beats both'])\n",
    "# Create the tf-idf feature matrix\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "# Create a search query and transform it into a tf-idf vector\n",
    "text = \"Brazil is the best\"\n",
    "vector = tfidf.transform([text])\n",
    "# Calculate the cosine similarities between the input vector and all other vectors\n",
    "cosine_similarities = linear_kernel(vector, feature_matrix).flatten()\n",
    "# Get the index of the most relevent items in order\n",
    "related_doc_indicies = cosine_similarities.argsort()[:-10:-1]\n",
    "# Print the most similar texts to the search query along with the cosine similarity\n",
    "print([(text_data[i], cosine_similarities[i]) for i in related_doc_indicies])\n",
    "# [\n",
    "#  (\n",
    "#  'Sweden is best', 0.6666666666666666),\n",
    "#  ('I love Brazil. Brazil!', 0.5163977794943222),\n",
    "#  ('Germany beats both', 0.0\n",
    "#  )\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a93e33-0533-48f8-908e-22965aa07dc7",
   "metadata": {},
   "source": [
    "# Using a Sentiment Analysis Classifier\n",
    "> The `transformers` library is an extremely popular library for NLP tasks and contains\n",
    "a number of easy-to-use APIs for training models or using `pretrained` ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d3a54d5-aa64-4a12-8a43-29334a663dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9998020529747009}] [{'label': 'POSITIVE', 'score': 0.9995730519294739}]\n"
     ]
    }
   ],
   "source": [
    "# If you want to classify the sentiment of some text to use as a feature or in downstream data analysis,\n",
    "# use the transformers library’s sentiment classifier.\n",
    "# Import libraries\n",
    "from transformers import pipeline\n",
    "# Create an NLP pipeline that runs sentiment analysis\n",
    "classifier = pipeline(\"text-classification\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "# Classify some text\n",
    "# (this may download some data and models the first time you run it)\n",
    "sentiment_1 = classifier(\"I hate machine learning! It's the absolute worst.\")\n",
    "sentiment_2 = classifier(\n",
    " \"Machine learning is the absolute\"\n",
    "\"bees knees I love it so much!\"\n",
    ")\n",
    "# Print sentiment output\n",
    "print(sentiment_1, sentiment_2)\n",
    "# [\n",
    "#  {\n",
    "#  'label': 'NEGATIVE',\n",
    "#  'score': 0.9998020529747009\n",
    "#  }\n",
    "# ]\n",
    "# [\n",
    "#  {\n",
    "#  'label': 'POSITIVE',\n",
    "#  'score': 0.9990628957748413\n",
    "#  }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f376731-e26b-42c6-8ec0-43a04c2b9753",
   "metadata": {},
   "source": [
    "# END of Chapter 6 --> Handling Text data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_Kaveh",
   "language": "python",
   "name": "gpu_kaveh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
